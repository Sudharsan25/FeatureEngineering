Life cycle of a data science project.

1. Data collection
2. Feature engineering
   -> Handling missing values(Cont data, Categorical data)
  
    => Types of Missing data:
        1. Missing completely at Random(MCAR)
            There is absolutely no relationship with data missing and any other values, observed or missing, within in the dataset.
            Eg: If two colums in a dataframe has more null values , then the first column's missing data has absolutely no relationship
            with the second column's missing data.
         2. Missing not at random(MNAR) or Systematic missing values
             There is some relationshoip present with the data missing and other values, observed or missing within the datset
         3. Missing at random(MAR)
             Data is missing because of random factors, like the data from surveys, etc,.
    => Techniques of Handling the Missing values (Numerical features)
        1. Mean/Median/Mode Replacement - Replacing the missing values by most frequent occurance of a variable. 
                When? Used when Missing at random (MAR)
                Advantages:
                    *Easy to implement (Robust to outliers)
                    *Faster way to obtain complete dataset
                Disadvantages:
                    *Change or distortion of original variance
                    *Impacts Correlation
                    
        2. Random sample Imputation - It consists of taking random observation from the dataset and we use this observation to replace Nan
                When? Used when data Missing completely at Random(MCAR)
                Advantages:
                    *Easy to Implement
                    *Very Less distortion in variance
                Disadvantages:
                    *Randomness won't work always            
                
        3. Capturing Nan values with a new feature - Used to create an extra feature to provide importancce to missing values
                When? Works well when data Missing not at random(MNAR)
                Advantages:
                    *Easy to implement
                    *Captures Importance of Missing values
                Disadvantages:
                    *Creates Additional features(more no.of features with nan values)- curse of dimensionality
                    
        4. End of distribution Imputation - replace missing data with values that are at the tails of the distribution of the variable
                When? Used when data Missing not at random(MNAR)
                Advantages:
                    *Easy to implement
                    *Captures the importance of missing values if there is one
                Disadvantages:
                    *Distorts the original distribution of the variable
                    *If no.of NA is big , it will mask true outliers in the distribution
                    *If no.of NA is small, the replaced values can be considered as outliers and pre processed in a subsquent feature engg
                    
        5. Arbitrary Imputation - replacing all occurrences of missing values (NA) within a variable with an arbitrary value
                Arbitrary values
                    =>Should not be more frequently present
                Advantages:
                    *Easy to Implement
                    *Captures importance of missing values
                Disadvantages:
                    *Distorts the original distribution of the variable
                    *Hard to decide which value to use(arbitrary)
    =>Techniques to handle missing values (Categorical features)
        1. Frequent category imputation - Replacing the nan values with the most frequent category
                Advantages:
                    *Easy to Implement
                    *Faster Way to Implement 
                Disadvantages:
                    *Since wee are using most frequent labels it may use them in an over represented way , if the nan values are more
                    *It distorts the relation of the most frequent label
        2. Adding new feature to capture Nan - Craeting a new variable to represent the presence of nan values (1 if nan and 0 if not nan)
                Advantages:
                    *Easy to implement
                    *Captures the importance of missing values
                Disadvantages:
                    *Increases the no.of columns if more features have Nan values
        3. If more Frequent Categories, Replace nan with a New category
                
   ->Handling Categorical Features     
       1. One hot encoding - categorical variables are converted into a form that could be provided to ML algorithms
               Advantages:
                   *Easy to implement
                   *Model applicable form
               Disadvantages:
                   *More no.of categories , more no.of columns is created
       2. One hot encoding for many categories - Instead of all categories, top 10 most frequent categories are one hot encoded
       
       3. Ordinal number encoding - Assinging a rank for the category depending on the importance or order of category
       
       4. Count or Frequency encoding - Replace the categories by the count of the observations that show that category in the dataset
               Advantages:
                   *Easy to use
                   *Not Increasing any feature space
               Disadvantages:
                   *It will provide the same weight if two categories have same frequency
       5. Target Guided Ordinal Encoding -  we calculate the mean of each categorical variable based on the output and then rank them from highest mean to lowest.
               
       6. Mean Encoding - Replace the category by it's mean
               Advantages:
                   *Straightforward to implement.
                   *Does not expand the feature space.
                   *Creates a monotonic relationship between categories and the target.
               Limitations of Mean encoding
                   *May lead to overfitting.
                   *May lead to a possible loss of value if two categories have the same mean as the targetâ€”in these cases, the same number replaces the original.
       7. Probability Ratio Encoding - for each category, we calculate the mean of the target = 1, which is the probability of the target being 1 (P(1)), and the probability of the target being 0 (P(0)).Finally, we calculate the ratio = P(1)/P(0), and we replace the categories by that ratio.


